---
title: "Business Intelligence Project"
output:
  github_document: 
    toc: yes
    toc_depth: 4
    fig_width: 6
    fig_height: 4
    df_print: default
editor_options:
  chunk_output_type: console
---

# Student Details

|                                              |                             |
|--------------------------------------------|----------------------------|
| **Student ID Number**                        | 119630,135844,131038,104135 |
| **Student Name**                             | beasts                      |
| **BBIT 4.2 Group**                           | A&B&C                       |
| **BI Project Group Name/ID (if applicable)** | beasts                      |

# Setup Chunk

**Note:** the following KnitR options have been set as the global defaults: <BR> `knitr::opts_chunk$set(echo = TRUE, warning = FALSE, eval = TRUE, collapse = FALSE, tidy = TRUE)`.

More KnitR options are documented here <https://bookdown.org/yihui/rmarkdown-cookbook/chunk-options.html> and here <https://yihui.org/knitr/options/>.

```{r setup, include=FALSE}
library(formatR)
knitr::opts_chunk$set(
  warning = FALSE,
  collapse = FALSE
)
```

# Load dataset

```{r}
insurance <- read.csv("data/insurance_info.csv")

```

# Exploratory Data Analysis

## Dimensions

```{r}
dim(insurance)
```

## Data Types

```{r}
sapply(insurance, class)
```

## Descriptive Statistics

### Measures of Frequency

```{r}
insurance_rating_freq <- insurance$rating
cbind(frequency = table(insurance_rating_freq),
      percentage = prop.table(table(insurance_rating_freq)) * 100)
```

### Measures of Central Tendency

```{r}
insurance_rating_mode <- names(table(insurance$rating))[
  which(table(insurance$rating) == max(table(insurance$rating)))
]
print(insurance_rating_mode)
```

## Measures of Distribution/Dispersion/Spread/Scatter/Variability

### Measure the distribution of the data for each variable

```{r}
summary(insurance)
```

### Measure the standard deviation of each variable

```{r}
library(e1071)

sapply(insurance[, c(6,8)], sd)
```

### Measure the variance of each variable

```{r}
sapply(insurance[, c(6,8)], var)
```

### Measure the kurtosis of each variable

```{r}
library(e1071)

sapply(insurance[, c(6, 8)], kurtosis, type = 2)

```

### Measure the skewness of each variable

```{r}
sapply(insurance[, c(6, 8)],  skewness, type = 2)
```

## Measures of Relationship

### Measure the covariance between variables

```{r}
insurance_cov <- cov(insurance[, c(6, 8)])
View(insurance_cov)

```

### Measure the correlation between variables

```{r}
insurance_cor <- cor(insurance[, c(6, 8)])
View(insurance_cor)
```

## Inferential Statistics

### Perform ANOVA

#### One-Way ANOVA

```{r}
insurance_one_way_anova <- aov(age ~ rating, data = insurance)
summary(insurance_one_way_anova)
```

#### Two-Way ANOVA

```{r}
insurance_two_way_anova <- aov(packagePrice ~ rating + age, # nolint
                                           data = insurance)
summary(insurance_two_way_anova)
```

## Qualitative Data Analysis

### Univariate plots

#### Create histograms

```{r}
insurance_age <- as.numeric(unlist(insurance[, 6]))
hist(insurance_age, main = names(insurance)[6])

```

#### Create Box and Whisker Plots for Each Numeric Attribute

```{r}
par(mar = c(1, 1, 1, 1))

boxplot(insurance[, 6], main = names(insurance)[6])
boxplot(insurance[, 2], main = names(insurance)[2])
boxplot(insurance[, 8], main = names(insurance)[8])
```

#### Create a Missingness Map to Identify Missing Data

```{r}
library(Amelia)

missmap(insurance, col = c("red", "grey"), legend = TRUE)
```

### Multivariate Plots 
#### Create a Correlation Plot 
```{r}
library(corrplot)

corrplot(cor(insurance[, c(6, 8)]), method = "circle")
```

# Data Imputation
## Handle missing values
### Are there missing values in the dataset?

```{r}
library(tidyverse)

anyNA(insurance)
```
### How many?

```{r}
library(naniar)

n_miss(insurance)
```

### What is the proportion of missing data in the entire dataset?

```{r}
prop_miss(insurance)
```

### How many missing values does each variable have?
```{r}
insurance %>% is.na() %>% colSums()
```

### What is the number and percentage of missing values grouped by each variable?

```{r}
miss_var_summary(insurance)
```

### What is the number and percentage of missing values grouped by each observation?

```{r}
miss_case_summary(insurance)
```

### Which variables contain the most missing values?

```{r}
gg_miss_var(insurance)
```

## Remove the observations with missing values

```{r}
insurance_obs <- insurance %>% filter(complete.cases(.))
dim(insurance_obs)
```


# Data transformation
## The Scale Basic Transform
### Before
```{r}
library(caret)

summary(insurance_obs)
insurance_obs_yield <- as.numeric(unlist(insurance_obs[, 8]))
hist(insurance_obs_yield, main = names(insurance_obs)[8])

model_of_the_transform <- preProcess(insurance_obs, method = c("scale"))
print(model_of_the_transform)
insurance_scale_transform <- predict(model_of_the_transform, insurance_obs)
```

### After
```{r}
summary(insurance_scale_transform)
insurance_obs_yield <- as.numeric(unlist(insurance_scale_transform[, 8]))
hist(insurance_obs_yield, main = names(insurance_scale_transform)[8])
```


## Center Data Transform

```{r}
summary(insurance_obs)
model_of_the_transform <- preProcess(insurance_obs, method = c("center"))
print(model_of_the_transform)
insurance_center_transform <- predict(model_of_the_transform, insurance_obs)
summary(insurance_center_transform)
```

## Standardize Data Transform
### Before

```{r}
summary(insurance_obs)
sapply(insurance_obs[, c(6,8)], sd)
model_of_the_transform <- preProcess(insurance_obs,
                                     method = c("scale", "center"))
print(model_of_the_transform)
insurance_standardize_transform <- predict(model_of_the_transform, insurance_obs) # nolint

```

### After

```{r}
summary(insurance_standardize_transform)
sapply(insurance_standardize_transform[, c(6,8)], sd)
```

## Normalize Data Transform

```{r}
summary(insurance_obs)
model_of_the_transform <- preProcess(insurance_obs, method = c("range"))
print(model_of_the_transform)
insurance_normalize_transform <- predict(model_of_the_transform, insurance_obs)
summary(insurance_normalize_transform)
```

## Box-Cox Power Transform 
### Before

```{r}
library(e1071)

summary(insurance_standardize_transform)

# Calculate the skewness before the Box-Cox transform
sapply(insurance_standardize_transform[, c(6,8)],  skewness, type = 2)
sapply(insurance_standardize_transform[, c(6,8)], sd)

model_of_the_transform <- preProcess(insurance_standardize_transform,
                                     method = c("BoxCox"))
print(model_of_the_transform)
insurance_box_cox_transform <- predict(model_of_the_transform,
                                       insurance_standardize_transform)
```

### After

```{r}
summary(insurance_box_cox_transform)

sapply(insurance_box_cox_transform[, c(6,8)],  skewness, type = 2)
sapply(insurance_box_cox_transform[, c(6,8)], sd)

# Calculate the skewness after the Box-Cox transform
sapply(insurance_box_cox_transform[, c(6,8)],  skewness, type = 2)
sapply(insurance_box_cox_transform[, c(6,8)], sd)
```
## Yeo-Johnson Power Transform 
### Before
```{r}
summary(insurance_standardize_transform)

# Calculate the skewness before the Yeo-Johnson transform
sapply(insurance_standardize_transform[, c(6,8)],  skewness, type = 2)
sapply(insurance_standardize_transform[, c(6,8)], sd)

model_of_the_transform <- preProcess(insurance_standardize_transform,
                                     method = c("YeoJohnson"))
print(model_of_the_transform)
insurance_yeo_johnson_transform <- predict(model_of_the_transform, # nolint
                                           insurance_standardize_transform)

```
### After
```{r}
summary(insurance_yeo_johnson_transform)

# Calculate the skewness after the Yeo-Johnson transform
sapply(insurance_yeo_johnson_transform[, c(6,8)],  skewness, type = 2)
sapply(insurance_yeo_johnson_transform[, c(6,8)], sd)

```

## PCA for Dimensionality Reduction
```{r}
summary(insurance_obs)

model_of_the_transform <- preProcess(insurance_obs, method =
                                       c("scale", "center", "pca"))

print(model_of_the_transform)
insurance_pca_dr <- predict(model_of_the_transform, insurance)

summary(insurance_pca_dr)
```

## ICA for Dimensionality Reduction on the Boston Housing Dataset
```{r}
summary(insurance_obs)

model_of_the_transform <- preProcess(insurance_obs,
                                     method = c("scale", "center", "ica"),
                                     n.comp = 3)
print(model_of_the_transform)
insurance_ica_dr <- predict(model_of_the_transform, insurance)

summary(insurance_ica_dr)
```

# Resampling methods
## Split the dataset
```{r}
train_index <- createDataPartition(insurance_obs$rating,
                                   p = 0.75,
                                   list = FALSE)
insurance_obs_train <- insurance_obs[train_index, ]
insurance_obs_test <- insurance_obs[-train_index, ]
```

## Single Format 
```{r}
library(arules)

transactions_single_format <-
  read.transactions("data/transactions_single_format.csv",
                    format = "single", cols = c(1, 2))

inspect(head(transactions_single_format))
print(transactions_single_format)
```
##  Basket Format

```{r}
library(arules)

transactions_basket_format_listings <-
  read.transactions("data/transactions_basket_format.csv",
                    format = "basket", sep = ",", cols = 2)
summary(transactions_basket_format_listings)  # Summary of the transactions
inspect(transactions_basket_format_listings)  # View the transactions
```

## Create a transaction data using the "basket format"

```{r}
transaction_data_insurance <-
  plyr::ddply(insurance_obs,
    c("insurance.name","services", "priceRage", "age", "preIllness"),
    function(df1) paste(df1$rating, collapse = ","))

View(transaction_data_insurance)

```

### Record only the `items` variable

```{r}
library(dplyr)

transaction_data_insurance <-
  transaction_data_insurance %>%
  dplyr::select("items" = V1)%>% 
  mutate(items = paste("{", items, "}", sep = ""))

View(transaction_data_insurance)
```

## Save the transactions in CSV format

```{r}
write.csv(transaction_data_insurance,
          "data/transactions_basket_format_insurance.csv",
          quote = FALSE, row.names = FALSE)
```

## Read the transactions from the CSV file

```{r}
tr_insurance <-
  read.transactions("data/transactions_basket_format_insurance.csv",
    format = "basket",
    header = TRUE,
    rm.duplicates = TRUE,
    sep = ","
  )

print(tr_insurance)
summary(tr_insurance)
```

## Basic EDA

```{r}
library(RColorBrewer)

itemFrequencyPlot(tr_insurance, topN = 10, type = "absolute",
                  col = brewer.pal(8, "Pastel2"),
                  main = "Absolute Item Frequency Plot",
                  horiz = TRUE,
                  mai = c(1, 1, 1, 0.9))
itemFrequencyPlot(tr_insurance, topN = 10, type = "relative",
                  col = brewer.pal(8, "Pastel2"),
                  main = "Relative Item Frequency Plot",
                  horiz = TRUE,
                  mai = c(1, 1, 1, 0.9))
```

## Create the association rules

```{r}
library(arulesViz)
association_rules_insurance <- apriori(tr_insurance,
                             parameter = list(support = 0.01,
                                              confidence = 0.05,
                                              maxlen = 10)) 

#Print the association rules to view the top 10 rules
summary(association_rules_insurance)
inspect(association_rules_insurance)
# To view the top 10 rules
inspect(association_rules_insurance[1:10])
plot(association_rules_insurance)

```
### Remove redundant rules 
```{r}
subset_rules <-
  which(colSums(is.subset(association_rules_insurance,
                          association_rules_insurance)) > 1)
length(subset_rules)
association_rules_no_reps <- association_rules_insurance[-subset_rules]

summary(association_rules_no_reps)
inspect(association_rules_no_reps)

write(association_rules_no_reps,
      file = "rules/association_rules_based_on_rating_code.csv")
```

## Find specific rules

```{r}
rating_association_rules <- # nolint
  apriori(tr_insurance, parameter = list(supp = 0.01, conf = 0.05),
          appearance = list(default = "rhs",
                            rhs = "2"))
inspect(head(rating_association_rules))
```

## Visualize the rules
### Filter rules with confidence greater than 0.85 or 85%
```{r}
rules_to_plot <-
  association_rules_insurance[quality(association_rules_insurance)$confidence > 0.08] # nolint
#Plot SubRules
plot(rules_to_plot)
plot(rules_to_plot, method = "two-key plot")
```

```{r}
top_10_rules_to_plot <- head(rules_to_plot, n = 10, by = "confidence")
plot(top_10_rules_to_plot, method = "graph",  engine = "htmlwidget")

#saveAsGraph(head(rules_to_plot, n = 1000, by = "lift"),
           # file = #"graph/association_rules_prod_no_reps.graphml")

```

### Filter top 20 rules with highest lift
```{r}
rules_to_plot_by_lift <- head(rules_to_plot, n = 20, by = "lift")
plot(rules_to_plot_by_lift, method = "paracoord")

plot(top_10_rules_to_plot, method = "grouped")
```


































